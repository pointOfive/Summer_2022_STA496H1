{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bayesian Importance Sampling\n",
    "\n",
    "## Scott Schwartz Intial Questions / Extended May 3 (2022)\n",
    "---\n",
    "\n",
    "0. For target distribution $p(\\theta|x)$ (a posterior), and proposal distribution $p(\\theta)$ (a prior), show that the normalized importance weights for importance sampling are proportional to $f(x|\\theta)$ (the likelihood).\n",
    "\n",
    "  > Hint: Monte Carlo integration of $g(\\theta) p(\\theta|x)$ can be based on importance sampling with the prior $p(\\theta)$ as the proposal distribution since\n",
    "  > $$\\int g(\\theta) p(\\theta|x) d\\theta = \\int g(\\theta)  \\frac{p(\\theta)}{p(\\theta)} p(\\theta|x) d\\theta $$\n",
    "\n",
    "1. Are there any conditions that the prior should satisfy to be a good proposal distribution?\n",
    "\n",
    "   > Hint: [Chapter 6.4.1 Importance Sampling (Givens/Hoeting)](https://librarysearch.library.utoronto.ca/permalink/01UTORONTO_INST/14bjeso/alma991106781097906196) is helpful here.\n",
    "\n",
    "2. What are the strengths of the unnormalized importance weights that might make them better for a given application context?  \n",
    "  \n",
    "   1. What precisely is the difference between unnormalized and normalized importance weights?\n",
    "   2. What is the computational difference between the mathematical forms of these importance weights? I.e., what can be said about the marginal likelihood with respect to unnormalized and normalized importance weights and what does that mean, computationally?\n",
    "\n",
    "  > Hint: the bias characterization in eq. 6.42 of [Chapter 6.4.1 Importance Sampling (Givens/Hoeting)](https://librarysearch.library.utoronto.ca/permalink/01UTORONTO_INST/14bjeso/alma991106781097906196) and the [general wisdom](https://en.wikipedia.org/wiki/Marginal_likelihood) that \"Unfortunately, marginal likelihoods are generally difficult to compute\" are the key considerations for all these questions.\n",
    "\n",
    "3. Importance Sampling is often presented as a variance reduction technique. Since our desire is to produce a (weighted) sample representation of the posterior, we do not immediately have an estimates for which variance reduction might apply; however, Bayesian credible intervals are based on percentile ranks within posterior samples, so might we expect variance reduction with respect to our estimates of Bayesian credible intervals? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bayesian Importance Sampling\n",
    "## Haining Tan, Sep 5 (2021) / Schwartz edits May 3 (2022)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "General idea of Importance Sampling: Given target distribution $p$ and proposal distribution $q$, we aim to estimate \n",
    "\n",
    "$$E_p[f(x)] = E_q\\left[f(x) \\frac{p(x)}{q(x)}\\right]$$\n",
    "\n",
    "Apply Importance Sampling with: \n",
    "\n",
    "- Target distribution: $p(\\theta|x)$, which is a posterior \n",
    "- Proposal distribution: $p(\\theta)$, which is a prior\n",
    "\n",
    "\n",
    "Assume $x$ is given ($\\theta$ conditioned on $x$), let $E[g(\\theta)]$ be the expectation of a function of $\\theta$ we want to estimate, we have\n",
    " \n",
    "\\begin{align*}\n",
    "    E[g(\\theta)] &= \\int g(\\theta) p(\\theta | x) \\,d\\theta \\\\\n",
    "    &= \\int g(\\theta) \\frac{p(\\theta | x)}{p(\\theta)} p(\\theta) \\,d\\theta\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $\\text{IW}(\\theta) = \\frac{p(\\theta | x)}{p(\\theta)}$; and, applying Bayes theorem to the  (unnormalized) importance weights $\\text{IW}(\\theta)$, \n",
    "\n",
    "$$p(\\theta|x) = \\frac{p(\\theta) f(x|\\theta)}{\\int_\\theta p(\\theta) f(x|\\theta)} \\propto p(\\theta) f(x|\\theta)$$\n",
    "\n",
    "so the (unnormalized) importance weights $\\text{IW}(\\theta) = \\frac{p(\\theta | x)}{p(\\theta)} \\propto  f(x|\\theta)$, which is just the likelihood.\n",
    "\n",
    "Now, proportional (unnormalized) importance weights $c \\times \\text{IW}(\\theta) = f(x|\\theta)$ cannot be used for importance sampling unless $c=\\sum_i \\text{IW}(\\theta^{(i)})$ so that $\\frac{1}{c}\\sum_i f(x|\\theta^{(i)}) = 1$, in which case the **normalized** importance weights $\\text{N-IW}(\\theta) = \\frac{1}{c}f(x|\\theta^{(i)})$ can indeed be used for importance sampling; although, $\\text{N-IW}(\\theta)$ will be biased compared to $\\text{IW}(\\theta)$ (although, this may not necessarily be a bad thing...).\n",
    "\n",
    "As seen above, the computational difference between $\\text{IW}(\\theta)$ and $\\text{N-IW}(\\theta)$ is that the former requires the marginal likelihood while the latter does not; and, \"[Unfortunately, marginal likelihoods are generally difficult to compute](https://en.wikipedia.org/wiki/Marginal_likelihood)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the question of the proposal distribution, for our context, according to [Chapter 6.4.1 Importance Sampling (Givens/Hoeting)](https://librarysearch.library.utoronto.ca/permalink/01UTORONTO_INST/14bjeso/alma991106781097906196), a good proposal distribution $p(\\theta)$ should satisfy:\n",
    "\n",
    "1. $\\text{IW}(\\theta) = \\frac{p(\\theta | x)}{p(\\theta)} \\propto  f(x|\\theta)$ is bounded and $p(\\theta)$ has heavier tails than $p(\\theta | x)$ \n",
    "    - which seems to imply that $p(\\theta)$ shaped and located coincidentally with the likelihood would produce excellent proposal coverage over the domain of relavant importance samples.\n",
    "\n",
    "2. For the estimate to have a low variance, $\\text{IW}(\\theta) = \\frac{p(\\theta | x)}{p(\\theta)}$ should be large only when $g(\\theta)$ is very small\n",
    "    - because $\\text{IW}(\\theta)$ is usually small when sampling from $p(\\theta)$, but since $E_{p(\\theta)}[\\text{IW}(\\theta)]=1$ clearly $\\text{Var}_{p(\\theta)}[\\text{IW}(\\theta)]$ can be large; so, if $g(\\theta)$ is small for whenever $\\text{IW}(\\theta)$ is large, then this excess variability will be supressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Adapted from \"Importance_Sampling.pdf\" from Haining Tan (Sep 2, 2021)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
