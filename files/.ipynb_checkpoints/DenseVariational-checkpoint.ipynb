{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pl6RsNOs87r3"
   },
   "source": [
    "---\n",
    "# Applications of TFP for regression with uncertainty\n",
    "##  Haining Tan, Sep 10 (2021) / Schwartz edits May 4-5 (2022)\n",
    "---\n",
    "\n",
    "This section re-implements the ideas in https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html\n",
    "\n",
    "* Specify a probabilistic model and minimize the negative log-likelihood\n",
    "\n",
    "* Deal with aleatoric uncertainty or/and epistemic uncertainty\n",
    "\n",
    "\n",
    "> 1.   Aleatoric uncertainty: variablity in y for any particular value of x (the uncertainty that has a known functional relationship with x)\n",
    "2.  Epistemic uncertainty: variablity in the estimated parameters of the model \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TriLrnZIhO6g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jUKB0cjPE2O"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "\n",
    "k = 1\n",
    "b = 5\n",
    "x_min = -50\n",
    "x_max = 50\n",
    "n = 150\n",
    "\n",
    "def aleatoric(x): # variablity function for a particular x\n",
    "  r = (x - x_min) / (x_max - x_min)\n",
    "  return 2 * r\n",
    "\n",
    "def generate_data(n):\n",
    "  x = (x_max - x_min) * np.random.rand(n) + x_min \n",
    "  noise = np.random.randn(n) * aleatoric(x)\n",
    "  y = (k * x * (1 + np.sin(x)) + b) + noise   # add some non-linearity and noise\n",
    "  x = x[..., np.newaxis] # convert to N * 1 matrix\n",
    "  return x, y\n",
    "\n",
    "x_train, y_train = generate_data(n)\n",
    "x_test, y_test = generate_data(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "w-FgbyyXfi7w",
    "outputId": "73f6e3b3-e168-4b3a-9f6d-0a681a23f8e0"
   },
   "outputs": [],
   "source": [
    "# Plot data\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_train, y_train, \"b.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrYY0buH_B3q"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def negloglik(y, py):\n",
    "  return - py.log_prob(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKC0jhjCWvUm"
   },
   "source": [
    "## Model 1: assume known constant variance and estimate line of best fit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_66lXZwc-TrA",
    "outputId": "44f8e137-8a2f-4dd7-fe2a-964129cf63e4"
   },
   "outputs": [],
   "source": [
    "# Using a DistributionLambda layer to output a (normal) distribution\n",
    "# where the mean is outputed by the second last layer\n",
    "# x -> mean -> Normal(mean, std=3)\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "                             tf.keras.layers.Dense(1), # approximate mean only: mean = kx (linear)\n",
    "                                                      # N * 1 -> N * 1 \n",
    "                             tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=3)) # constant std=3\n",
    "])                                                                                    \n",
    "model1.compile(optimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik)\n",
    "model1.fit(x_train,y_train, epochs=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "sFh3vmmv-VmX",
    "outputId": "409a6b24-fb0e-4175-e4ef-d14666879de5"
   },
   "outputs": [],
   "source": [
    "pred_m = model1(x_test).mean()\n",
    "pred_std = model1(x_test).stddev()  # should be constant\n",
    "pred_ub = pred_m + 2 * pred_std     # upper bound\n",
    "pred_lb = pred_m - 2 * pred_std     # lower bound\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'b.', label=\"train\")\n",
    "plt.plot(x_test, pred_m, \"r\", label = \"test\")\n",
    "plt.plot(x_test, pred_ub, \"g\", label = \"+2 std\")\n",
    "plt.plot(x_test, pred_lb, \"g\", label = \"-2 std\")\n",
    "plt.title(\"Regression with no uncertainty estimation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUA5WhP3pRta"
   },
   "source": [
    "Obviously, the model can not address the variability for different inputs appropritely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spy9D28hq4TA"
   },
   "source": [
    "## Model 2: estimate line of best fit only AND aleatoric (residual) uncertainty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3i7XFFKu5eu",
    "outputId": "983262fa-65cd-48f0-e54c-05f6630c1007"
   },
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(2), # N * 1 -> N * 2 \n",
    "  # estimate both mean and variance: mean = b1 + k1*x, std = b2 + k2*x (both linear) \n",
    "  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,:1], scale=t[:,1:]))\n",
    "])\n",
    "model2.compile(optimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik)\n",
    "model2.fit(x_train,y_train, epochs=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "bfqkxKO0mgrf",
    "outputId": "bd89265d-268f-4251-ad7d-bd29b5e67c9c"
   },
   "outputs": [],
   "source": [
    "pred_m = model2(x_test).mean()\n",
    "pred_std = model2(x_test).stddev() \n",
    "pred_ub = pred_m + 2 * pred_std     # upper bound\n",
    "pred_lb = pred_m - 2 * pred_std     # lower bound\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'b.', label=\"train\")\n",
    "plt.plot(x_test, pred_m, \"r\", label = \"test\")\n",
    "plt.plot(x_test, pred_ub, \"g\", label = \"+2 std\")\n",
    "plt.plot(x_test, pred_lb, \"g\", label = \"-2 std\")\n",
    "plt.title(\"Regression with consideration for aleatoric uncertainty\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4ZDusg3uiYS"
   },
   "source": [
    "Model 2 can capture the variablity in y for any particular x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNM3EtzVyEvp"
   },
   "source": [
    "## Model 3: Epistemic uncertainty only\n",
    "\n",
    "* Assume model parameter $\\theta \\sim $ Prior\n",
    "\n",
    "* Approach: use a `DenseVariational` layer instead of a fitting a (constant scalar) `Dense` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMWTzGCSHA7N"
   },
   "source": [
    "Model 3 can capture the variablity of the estimated parameters of the proposed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FpqM74VyDpx"
   },
   "outputs": [],
   "source": [
    "# Make posterior/prior function\n",
    "\n",
    "# Specify the surrogate (variational inference distribution) posterior \n",
    "# over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "  n = kernel_size + bias_size\n",
    "  c = np.log(np.expm1(1.))\n",
    "  return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(2 * n, dtype=dtype), # \"trainable\"\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "          tfd.Normal(loc = t[..., :n],\n",
    "                     scale = 0.2 + tf.nn.softplus(c + t[..., n:])), # std                     \n",
    "                     #https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Normal\n",
    "          reinterpreted_batch_ndims=1)),\n",
    "  ])\n",
    "\n",
    "# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "  n = kernel_size + bias_size # shape beta_1 and beta_0\n",
    "  return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(n, dtype=dtype), # theta_0\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "          tfd.Normal(loc=t, scale=1),\n",
    "          reinterpreted_batch_ndims=1)),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3InPX_a4_gV",
    "outputId": "31f1af41-8295-4aa0-807c-3c19e2985b53"
   },
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential([\n",
    "  tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/x_train.shape[0]), \n",
    "  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,:1], scale=3))\n",
    "])\n",
    "model3.compile(optimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik)\n",
    "model3.fit(x_train, y_train, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Initial Comments on \"Model 3\"\n",
    "## Schwartz Sep 15 (2021) / revised May 4 (2022)\n",
    "---\n",
    "\n",
    "The prior and likelihood specification given above is \n",
    "\n",
    "- $p(\\theta) = MVN(\\theta_0,I)$ with $f(\\theta_0) \\propto 1$\n",
    "- $p(y|\\theta,x) = N(\\beta_0 + \\beta_1 x, 3)$ with $\\theta = (\\beta_0, \\beta_1)$\n",
    "\n",
    "which admit posterior approximation using [*mean-field* *variational inference*](https://math.stackexchange.com/questions/308149/what-is-the-meaning-of-mean-field) \n",
    "\n",
    "> i.e., approximating $p(\\theta|y,x)$ with the (*mean-field*) independent specification $q(\\theta) = q_{\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}(\\theta) = N(\\theta | \\boldsymbol{\\mu},\\text{diag}(\\boldsymbol{\\sigma}))$\n",
    "\n",
    "by minimizing $ KL [ \\;q(\\theta) \\;|| \\;p(\\theta|y) \\;]$ (where dependence on $x$ will henceforth be omitted from the notation for brevity).\n",
    "\n",
    "This minimization can be achieved by instead maximizing the so-called *Evidence Lower BOund* (ELBO) since the objectives\n",
    "\n",
    "$$\\min_q KL[\\, q(\\theta) \\, || \\, p(\\theta|y) \\,] \\quad \\text{ and } \\quad \\max_q E_{q(\\theta)}[\\log p(y|\\theta)] - KL[\\, q(\\theta) \\, || \\, p(\\theta) \\,]$$\n",
    "\n",
    "are equivalent as seen from\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\text{a constant}}{\\log p(y)} = {} & \\int \\log p(y) q(\\theta) d\\theta \\\\\n",
    "= {} & \\int \\log \\left( \\frac{p(\\theta,y)}{p(\\theta|y)} \\frac{q(\\theta)}{q(\\theta)}\\right) q(\\theta) d\\theta = \\int \\log \\left(\\frac{p(y|\\theta)p(\\theta)}{p(\\theta|y)} \\frac{q(\\theta)}{q(\\theta)}\\right) q(\\theta) d\\theta \\\\\n",
    "= {} & \\overbrace{\\underbrace{\\int \\log p(y|\\theta) q(\\theta) d\\theta }_{E_{q(\\theta)}[\\log p(y|\\theta)]}  - KL[\\, q(\\theta) \\, || \\, p(\\theta) \\,]}^{\\text{ELBO}} + KL[\\, q(\\theta) \\, || \\, p(\\theta|y) \\,] \n",
    "\\end{align*}\n",
    "\n",
    "The above equation is the precise manifestation of the fact that the injection of the alien term $q(\\theta) \\approx p(\\theta|y)$ is not coherrent with the probability structure $p(y,\\theta)$; but, $\\log p(y)$ can nonetheless be triangulated in terms of the variational distribution $q(\\theta)$ as $E_{q(\\theta)}[\\log p(y|\\theta)]$ expressed with the following adjustment terms:\n",
    "\n",
    "1. $\\log p(y) = E_{p(\\theta)}[\\log p(y|\\theta)] \\leq E_{q(\\theta)}[\\log p(y|\\theta)]$\n",
    "\n",
    "  - so the term $\\;- KL[\\, q(\\theta) \\, || \\, p(\\theta) \\,]\\;$ corrects for performing the wrong integral; however,\n",
    "  - this correction is only accurate in the case that $q(\\theta) = p(\\theta|y)$ and over-corrects if $q(\\theta) \\approx p(\\theta|y)$\n",
    "  \n",
    "\n",
    "2. $q(\\theta) \\approx p(\\theta|y)$ \n",
    "\n",
    "  - so $KL[\\, q(\\theta) \\, || \\, p(\\theta|y) \\,]$ re-corrects for the initial over-adjustment \n",
    "  - and disappears when $q(\\theta)$ perfectly approximates $p(\\theta|y)$\n",
    "\n",
    "\n",
    "It is intereting to note that the *Variational inference* technique of maximizing the ELBO actually acts according to the usual Bayesian mechanism of resolving the competing influences of the *likelihood* and the *prior*.\n",
    "I.e., better approximations $q(\\theta)$ of $p(\\theta|y)$, i.e., smaller values for $KL[\\, q(\\theta) \\, || \\, p(\\theta|y) \\,]$, are the result of the choice of $q(\\theta)$ which optimally balances between\n",
    "\n",
    "$$\\Large \\underset{\\text{the influence of the likelihood}}{\\max E_{q(\\theta)}[\\log p(y|\\theta)]} \\quad\\quad\\quad \\underset{\\text{the influence of the prior}}{\\min KL[\\, q(\\theta) \\, || \\, p(\\theta) \\,]}$$\n",
    "\n",
    "---\n",
    "\n",
    "> - The $\\text{ELBO}(q)$ may be equivalently defined in terms of only expectations (and no KL-terms) since, using ***Jensen's inequality***\n",
    ">\n",
    ">   $$ \n",
    "\\begin{align*}\n",
    "\\log p(y) = {} & \\log \\int p(y,\\theta) \\frac{q(\\theta)}{q(\\theta)} d\\theta \\quad \\quad \\longrightarrow  &={}&  \\log \\int  p(y|\\theta)\\frac{p(\\theta)}{q(\\theta)} q(\\theta) d\\theta \\\\\n",
    "= {} & \\log E_{q(\\theta)}\\left[ p(y,\\theta)\\frac{1}{q(\\theta)} \\right] \\quad \\; \\longrightarrow  &={}&  \\log E_{q(\\theta)}\\left[ p(y|\\theta)\\frac{p(\\theta)}{q(\\theta)} \\right]\\\\\n",
    "\\geq {} & E_{q(\\theta)}\\left[ \\log \\frac{p(y,\\theta)}{q(\\theta)} \\right] \\quad \\quad \\;\\,\\; \\longrightarrow &={} & E_{q(\\theta)}\\left[ \\log \\left(p(y|\\theta)\\frac{p(\\theta)}{q(\\theta)}\\right) \\right] \\\\\n",
    "= {} & \\underbrace{E_{q(\\theta)}[ \\log p(y,\\theta)] - E_{q(\\theta)}[ \\log q(\\theta)]}_{\\text{ELBO}(q)} &={} & \\underbrace{E_{q(\\theta)}[ \\log p(y|\\theta)] - KL[\\, q(\\theta) \\, || \\, p(\\theta) \\,]}_{\\text{ELBO}(q)}\n",
    "\\end{align*}\n",
    "$$\n",
    ">\n",
    ">- And the $\\text{ELBO}(q)$ is often just derived directly from the desired Bayesian optimization\n",
    ">\n",
    ">   $$\n",
    "\\begin{align*}\n",
    "KL[\\, q(\\theta) \\, || \\,  p(\\theta|y) \\, ] = {} & \\int \\log \\frac{q(\\theta)}{p(\\theta|y)}q(\\theta)d\\theta\\\\\n",
    "= {} & \\int \\log \\frac{q(\\theta)p(y)}{p(y|\\theta)p(\\theta)}q(\\theta) d\\theta\\\\\n",
    "= {} & \\log p(y) + \\int \\log \\frac{q(\\theta)}{p(\\theta)}q(\\theta) d\\theta - \\int \\log p(y|\\theta) q(\\theta) d\\theta \\\\\n",
    "= {} & \\log p(y) + \\underbrace{KL[\\, q(\\theta) \\, || \\,  p(\\theta) \\, ] - E_{q(\\theta)}[ \\log p(y|\\theta)]}_{-\\text{ELBO}(q)} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "7utF__oWDYKc",
    "outputId": "7440b438-8289-4b07-a6c7-b2ced0877104"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'b.', label=\"train\")\n",
    "for i in range(50):\n",
    "  pred = model3(x_test)\n",
    "  plt.plot(x_test, pred.mean(), 'r', linewidth=0.5)\n",
    "plt.title(\"Regression with consideration for epistemic uncertainty\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comments on Fitting \"Model 3\" with TensorFlow\n",
    "## Schwartz Sep 15 (2021) / revised May 5 (2022)\n",
    "---\n",
    "\n",
    "The subsequent code below will repeat the model fitting process already completed above; but, adds...\n",
    "1. code comment annotations of the details of the model and model fitting process\n",
    "2. \"callbacks\" to increase visibility of the loss function components driving model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Sep 15 (2021) / added here May 5 (2022)\n",
    "\n",
    "n = x_train.shape[0] # although, this was previously just been directly defined as `n=150` above\n",
    "# `x_train` is the input to this model and affine transformed (without nonlinearity) in the usual NN manner\n",
    "model3 = tf.keras.Sequential([\n",
    "    \n",
    "  # https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational\n",
    "  # `1` means one unit with a bias unless `use_bias=False`; and prior / variational distributions are \n",
    "  # \"Python callables taking tf.size(kernel), tf.size(bias)\" so `use_bias=False` induces `bias_size=0`\n",
    "  # This layer adds a bias to a linear transformation of (scalar) x -- i.e., a slope coefficient\n",
    "  tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, activation=None, # activation made explicit\n",
    "                              kl_weight=1/n),\n",
    "  # `kl_weight=1/n` is correct; but, it takes \"care\" to understand why as this is  \n",
    "  # an implementation dependent specification that needs to be explicitly clarified\n",
    "\n",
    "  # https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda  \n",
    "  # this layer adds the bias and the scaled input x_train to define the normal loc=mean \n",
    "  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,:1], scale=3)) # constant std=3\n",
    "])\n",
    "\n",
    "# `x_train` into the above NN outputs a distribution with a log-likelihood attribute\n",
    "# so `x_train` defines the log pdf which can be evaluated for `y` and this is the loss\n",
    "def negloglik(y, py):\n",
    "  return -py.log_prob(y)\n",
    "model3.compile(optimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik)\n",
    "# except... it's not the only loss... this is just the expected -log-pdf under the variational distribution...\n",
    "# there's also the KL-divergence term (which needs to be minimized as well)...\n",
    "# this is stored in the model's `.losses` attribute, which is different than `loss=negloglik`...\n",
    "# https://keras.io/api/losses/#the-addloss-api\n",
    "# \"These losses are cleared by the top-level layer at the start of each forward pass\"\n",
    "# \"So layer.losses always contain only the losses created during the last forward pass.\"\n",
    "#\n",
    "# So when the data passes through the `DenseVariational` layer the KL-term is added to `layer.losses`\n",
    "# and this will then be added (automatically) to `loss=negloglik`...\n",
    "#\n",
    "# but \"how\" is actually a question..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comments on the Underdocumented Parameter `kl_weight` \n",
    "## Schwartz Dec 20 (2021) / revised May 5 (2022)\n",
    "---\n",
    "\n",
    "The KL-divergence term is [calculated once for each minibatch](https://github.com/tensorflow/probability/blob/v0.15.0/tensorflow_probability/python/layers/dense_variational_v2.py#L113-L119) when the `DenseVariational​` `call` function is triggered during forward propegation, adding a (single) KL-term (i.e., a comparison of two distributions) to the `.looses​` property of the network.  The calculation itself is not dependent upon `batch_size` (or the data) in the current context, but will be [an average](https://github.com/tensorflow/probability/blob/f3777158691787d3658b5e80883fe1a933d48989/tensorflow_probability/python/layers/dense_variational_v2.py#L155-L158) in [some circumstances](https://github.com/tensorflow/probability/issues/651#issuecomment-616816643).\n",
    "\n",
    "As noted [here](https://github.com/tensorflow/probability/issues/396#issuecomment-496182549), defined [here](https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/engine/training.py#L711-L715), and discussed at length [here](https://stackoverflow.com/questions/61314548/where-exactly-are-the-kl-losses-used-after-the-forward-pass), individual KL-divergence terms are added to the loss, which can be [either scalar or per observation](https://stackoverflow.com/questions/63390725/should-the-custom-loss-function-in-keras-return-a-single-loss-value-for-the-batc). E.g., our current `negloglik` (from the [TF blog example](https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html)) is per observation, \n",
    "so the KL-divergence term is added to each observation and then the average (of observations of log loss plus KL penalty) is taken over the `batch_size`; whereas, the (more traditional) `mse_loss` is already averaged and so is already a scalar, so the KL penalty would just be added to that average directly.  \n",
    "\n",
    "- So for the per observation `negloglik`, where $n=B\\times \\text{batch_size}$\n",
    "  \n",
    "  \\begin{align*}\\frac{1}{B}\\sum_{b=1}^{B} \\frac{ \\sum_{j=1}^{\\text{batch_size}} \\left( \\log p(y_{bj}|x_{bj},\\theta) + \\frac{1}{n} KL(q|p) \\right)}{\\text{batch_size}} = {} & \\frac{\\sum_{b=1}^{B} \\sum_{j=1}^{\\text{batch_size}} \\log p(y_{bj}|x_{bj},\\theta) }{n} + \\frac{KL(q|p)}{n}\\\\\n",
    "\\approx {} & E_{q(\\theta)}[\\log p(y_{bj}|x_{bj},\\theta)] + \\frac{KL(q|p)}{n}\\\\\n",
    "\\overset{\\times n}{\\propto} {} & E_{q(\\theta)}[\\log p(\\mathbf{y}|\\mathbf{x},\\theta)] + KL(q|p)\n",
    "\\end{align*}\n",
    "\n",
    "  so `kl_weight=1/n` is correct since it keeps the correct 1:1 ratio between the expected log-loss term (**over all observations** $\\mathbf{y}$) and the KL-divergence term.\n",
    "\n",
    "- While (similarly) for `mse_loss`, the process would be\n",
    "\n",
    "  $$\\frac{1}{B}\\sum_{b=1}^{B} \\left(\\text{batch_MSE} + \\frac{1}{n}KL(q|p) \\right) \\approx \\text{MSE} + \\frac{KL(q|p)}{n}$$\n",
    "\n",
    "  so `kl_weight=1/train_size` would now keep the a 1:1 ratio between the `SSE_loss` (i.e., not `mse_loss`) and the KL-divergence term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "# It turns out it takes some work to get at batch level visibility into -logpdf + KL, as discussed at length, here:\n",
    "# https://stackoverflow.com/questions/47079111/create-keras-callback-to-save-model-predictions-and-targets-for-each-batch-durin/60787957#60787957\n",
    "\n",
    "# For example, \"out of the box\" capability only gives `loss = negloglik + .losses` for each batch\n",
    "# https://stackoverflow.com/questions/65383500/tensorflow-keras-keep-loss-of-every-batch\n",
    "batch_total_loss_callback = tf.keras.callbacks.LambdaCallback( \n",
    "  on_batch_end = lambda batch,logs: \n",
    "  print(\"\\nbatch \"+str(batch) + \" (loss = -logpdf+KL): \" + str(logs[\"loss\"]), end='')\n",
    ")\n",
    "\n",
    "# Further, the `self.model` attribue available to callbacks as documented here\n",
    "# https://www.tensorflow.org/guide/keras/custom_callback#usage_of_selfmodel_attribute\n",
    "# does not provide the full model access that it suggests\n",
    "# https://stackoverflow.com/questions/69802548/how-to-print-keras-tensor-values\n",
    "# (and again see the \"create-keras-callback-to-save-model-predictions-and-targets-for-each-batch-durin/\" link above)\n",
    "class batch_model_losses_attribute_callback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs={}): # batch is an integer index\n",
    "        print('\\nEvaluation of self.model is \"lazy/eager\": '+str(self.model.losses[0])) \n",
    "        # which will show that this attribute doesn't actually have access to the computation on the data\n",
    "        # that was instantiated by the batch and completed.\n",
    "# Now, as the \"how-to-print-keras-tensor-values\" indicates, `self.model.losses[0]` could be actualized\n",
    "# but this would require knowing the batch on each training step, and this can't be done with callback tools\n",
    "# so instead we can just rerun the model and gather the losses as follows\n",
    "class epoch_model_losses_attribute_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None): # batch is an integer index\n",
    "        print(\"\\n-logpdf: \" +str(tf.math.reduce_mean(negloglik(y_train[:, np.newaxis], model3(x_train)))))\n",
    "        print(\"+KL: \" +str(model3.losses[0]))\n",
    "        print(\"But now do this again\")\n",
    "        print(\"-logpdf: \" +str(tf.math.reduce_mean(negloglik(y_train[:, np.newaxis], model3(x_train)))))\n",
    "        print(\"+KL: \" +str(model3.losses[0])+\"\\n\")\n",
    "# However, such re-runs produces difference results since `DenseVariational is \"stochastic\"\n",
    "# so this is not true tracking of the -logpdf + KL components across batches\n",
    "\n",
    "# So for callbacks (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) to help us \n",
    "# track the components -logpdf + KL we're going to need to do a little more as the above/below demonstrate\n",
    "model3.compile(optimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik)\n",
    "model3.fit(x_train, y_train[:, np.newaxis], \n",
    "# `y_train[:, np.newaxis]` rather than `y_train` because\n",
    "# `negloglik(y_train[:, np.newaxis], model3(x_train))` works but `negloglik(y_train, model3(x_train))` doesn't\n",
    "           validation_data=(x_train, y_train[:, np.newaxis]),\n",
    "           epochs=1, verbose=True, \n",
    "           callbacks=[batch_total_loss_callback, epoch_model_losses_attribute_callback(), \n",
    "                      batch_model_losses_attribute_callback()])  \n",
    "# regarding default `batch_size=None` the documentation says \"if unspecified, batch_size will default to 32.\"\n",
    "# https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "# Note that the second time this is run `batch_model_losses_attribute_callback()` uses the last value above\n",
    "# I.e., as stated above, it doesn't have access to the computation instantiated by a batch of data\n",
    "# Also take note that the KL-divergence computation can(not) be negative(!!??): this will be addressed below\n",
    "model3.fit(x_train, y_train[:, np.newaxis], \n",
    "           validation_data=(x_train, y_train[:, np.newaxis]),\n",
    "           epochs=1, verbose=True, \n",
    "           callbacks=[batch_total_loss_callback, epoch_model_losses_attribute_callback(), \n",
    "                      batch_model_losses_attribute_callback()])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "# To get the visibiity of the -logpdf + KL components, rather than following the callback based solutions in\n",
    "# https://stackoverflow.com/questions/47079111/create-keras-callback-to-save-model-predictions-and-targets-for-each-batch-durin/60787957#60787957\n",
    "# We gain access to both `loss=negloglik` and `.losses` terms by creating a custom training step\n",
    "# by slightly modify the default `train_step` code, which will then allow us to use the `callbacks` functionality\n",
    "# https://keras.io/guides/customizing_what_happens_in_fit/\n",
    "# https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L753\n",
    "\n",
    "# I.e., we will make the \"logs\" which `callbacks` has access to include the desired -logpdf + KL components \n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "nll_tracker = tf.keras.metrics.Mean(name=\"negloglike\")\n",
    "KL_tracker = tf.keras.metrics.Mean(name=\"KL-div\")\n",
    "class ModelPrintingLosses(tf.keras.Model):\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    # We list our `Metric` objects here so that `reset_states()` can be\n",
    "    # called automatically at the start of each epoch or at the start of `evaluate()`.\n",
    "    # If you don't implement this property, you have to call \n",
    "    # `reset_states()` yourself at the time of your choosing.\n",
    "    return [loss_tracker, nll_tracker, KL_tracker]\n",
    "\n",
    "  # I.e., we just simply redefine the train step of the keras .fit procedure to add -logpdf + KL to the logs\n",
    "  def train_step(self, data):\n",
    "    x, y = data # `sample_weight` removed\n",
    "    # Run forward pass.\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred = self(x, training=True)\n",
    "      loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses) # `sample_weight` removed\n",
    "\n",
    "    if self.loss and y is None:\n",
    "      raise TypeError(\n",
    "          f'Target data is missing. Your model has `loss`: {self.loss}, '\n",
    "          'and therefore expects target data to be passed in `fit()`.')\n",
    "    # Run backwards pass.\n",
    "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
    "    #self.compiled_metrics.update_state(y, y_pred) # `sample_weight` removed\n",
    "    \n",
    "    # these will return running averages, reset for each new epoch\n",
    "    nll = self.loss(y, y_pred)\n",
    "    nll_tracker.update_state(nll)\n",
    "    kld = self.losses[0]\n",
    "    KL_tracker.update_state(kld)\n",
    "    loss_tracker.update_state(tf.reduce_mean(nll)+kld)\n",
    "    # so while the batch calculations sum, the running averages need not\n",
    "\n",
    "    # Collect metrics to return\n",
    "    return_metrics = {}\n",
    "    for metric in self.metrics:\n",
    "      result = metric.result()\n",
    "      if isinstance(result, dict):\n",
    "        return_metrics.update(result)\n",
    "      else:\n",
    "        return_metrics[metric.name] = result\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "# Now the `callbacks` functionality will work for us as we can just access our desired -logpdf + KL components\n",
    "# which are now just added to the logs attribute available to the `callbacks` functionality\n",
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "  tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/n),\n",
    "  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,:1], scale=3)) \n",
    "])\n",
    "\n",
    "# we can now access the components of the losses we care about since \n",
    "# our `ModelPrintingLosses` custom `tf.keras.Model` makes them available as part of the \"logs\"\n",
    "loss_callback = tf.keras.callbacks.LambdaCallback( \n",
    "  on_batch_end=lambda batch,logs: \n",
    "  print(\"\\nbatch \"+str(batch) + \": -logpdf+KL \" + str(np.float32(logs[\"loss\"])) +\n",
    "        \" -logpdf \" + str(np.float32(logs[\"negloglike\"])) +\n",
    "        \" KL-penalty \" + str(np.float32(logs[\"KL-div\"])), end='\\n'*(batch==4))\n",
    ")\n",
    "\n",
    "# here's another way to do this where they're saved: same as above but not `LambdaCallback`\n",
    "# https://stackoverflow.com/questions/42392441/how-to-record-val-loss-and-loss-per-batch-in-keras\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.nll = []\n",
    "        self.kl = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.nll.append(logs.get('negloglike'))\n",
    "        self.kl.append(logs.get('KL-div'))\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(1,))\n",
    "model3 = ModelPrintingLosses(input, model3(input))\n",
    "model3.compile(optimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik)\n",
    "# for some reason `verbose=True` is overwritting the \"batch 0\" callback; but, (see below)\n",
    "# the information is the same so I'll not try to fix that at this point\n",
    "batch_history = LossHistory()\n",
    "history = model3.fit(x_train, y_train[:, np.newaxis], epochs=3, verbose=True, \n",
    "                     callbacks=[loss_callback, batch_history])\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches above\n",
    "batch_history.loss[10], batch_history.nll[10], batch_history.kl[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[(k,v[0]) for k,v in history.history.items()]\n",
    "# only matches the final accumulation over epochs, so that's why `batch_history` is needed\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comments on the `DenseVariational` Implementation\n",
    "## Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "---\n",
    "\n",
    "Rather than pursuing the $\\min_q KL[\\, q(\\theta) \\, || \\, p(\\theta|y) \\,]$ objective directly,\n",
    "the TensorFlow *variational inference* framework is instead seen to pursue  \n",
    "\n",
    "$$\\max_q E_{q(\\theta)}[\\log p(y|\\theta)] - KL[\\, q(\\theta) \\, || \\, p(\\theta) \\,] = \\min_q KL[\\, q(\\theta) \\, || \\, p(\\theta) \\,] - E_{q(\\theta)}[\\log p(y|\\theta)]$$\n",
    "\n",
    "As indicated in the [documentation](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational), the `call` and `_make_kl_divergence_penalty` functions can be seen in the [source code](https://github.com/tensorflow/probability/blob/v0.13.0/tensorflow_probability/python/layers/dense_variational_v2.py#L26-L145) to instantiate a layer of posterior distributions whenever the `DenseVariational` layer is passed input (regardless of the input batch size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "# source code line 121: `q = self._posterior(inputs)`\n",
    "\n",
    "DVL = tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, \n",
    "                                           kl_weight=1/x_train.shape[0])\n",
    "batch_size=10\n",
    "DVL(np.array(batch_size*[[0.]])) # initialize `call(.)`\n",
    "DVL._posterior(np.array(batch_size*[[0.]])) # size 2 since this includes beta_0 and \\beta_1 (offset b and weight w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, KL-divergence either calculates exactly or \"approximately\" using a single sample from  $q(\\theta)$, i.e.,\n",
    "\n",
    "$$KL[\\, q(\\theta) \\, || \\,  p(\\theta) \\, ] = \\int log \\frac{q(\\theta)}{p(\\theta)} q(\\theta) d \\theta \\quad \\text{ or } \\quad KL[\\, q(\\theta) \\, || \\,  p(\\theta) \\, ] \\approx log \\frac{q(\\theta^*)}{p(\\theta^*)}, \\text{with } \\theta^* \\sim q(\\theta)$$\n",
    "\n",
    "depending on the setting of `use_exact_kl=[True|False]`, and adds it to the model loss:\n",
    "\n",
    "- source code line 123: `self.add_loss(self._kl_divergence_fn(q, r))`\n",
    "\n",
    "   where `self._kl_divergence_fn(q, r)` is either\n",
    "    \n",
    "   1. source code line 156: `kl_divergence_fn = kullback_leibler.kl_divergence`\n",
    "\n",
    "       or\n",
    "       \n",
    "   2. source code line 158: `def kl_divergence_fn(distribution_a, distribution_b):` \n",
    "   \n",
    "      which uses \n",
    "      \n",
    "      ```\n",
    "      # source code line 151: test_points_fn=tf.convert_to_tensor\n",
    "      # source code line 159: z = test_points_fn(distribution_a)\n",
    "      # source code line 161: distribution_a.log_prob(z) - distribution_b.log_prob(z)\n",
    "      ```\n",
    "      \n",
    "      based on the realized actualization of the distribuitonal layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "print(DVL._posterior(np.array(batch_size*[[0.]])))\n",
    "print(tf.convert_to_tensor(DVL._posterior(np.array(batch_size*[[0.]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, the `tf.convert_to_tensor` of a distributional layer defaults to the `.sample()` method of the distribution (e.g., as opposed to the `.mean()` method):\n",
    "\n",
    "- source code line 125: `w = tf.convert_to_tensor(value=q)`\n",
    "      \n",
    "- and [by default](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda) `convert_to_tensor_fn=tfp.distributions.Distribution.sample`\n",
    "\n",
    "Indeed, all of this realization and instantiation occurs automatically simply by calling the a `DenseVariationalLayer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "# E.g., try changing to `scale = 0.2 + ...` with `scale = 200 + ...` in `posterior_mean_field` definition above\n",
    "# and see if `w*input+b` is based on `posterior_mean_field` or `prior_trainable`\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "  n = kernel_size + bias_size\n",
    "  c = np.log(np.expm1(1.))\n",
    "  return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(2 * n, dtype=dtype), # \"trainable\"\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "          tfd.Normal(loc = t[..., :n],\n",
    "                     scale = 0.2 + tf.nn.softplus(c + t[..., n:])), # std                     \n",
    "                     #scale = 200 + tf.nn.softplus(c + t[..., n:])), # for experimenting\n",
    "                     #https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Normal\n",
    "          reinterpreted_batch_ndims=1)),\n",
    "  ])\n",
    "\n",
    "# try changing the the nubmer of nodes?\n",
    "nodes = 1\n",
    "DVL = tfp.layers.DenseVariational(nodes, posterior_mean_field, prior_trainable, kl_weight=1/x_train.shape[0])\n",
    "batch_size = 10 # try changing this as well\n",
    "print(DVL(np.array(batch_size*[[0.]]))) # try changing the input data `[[0.]]`\n",
    "print(DVL.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Dec 20 (2021) / revised May 6 (2022)\n",
    "\n",
    "# So to summarize\n",
    "\n",
    "yhat = model3(x_train)\n",
    "print('-logpdf is actually average -logpdf, which is why we use the `kl_weight`, i.e., (1/n)*KL')\n",
    "print(negloglik(y_train[:,np.newaxis], yhat).numpy().mean(), \"not\", \n",
    "      negloglik(y_train[:,np.newaxis], yhat).numpy().sum())\n",
    "\n",
    "print(\"-logpdf (obviously) is not KL, and KL is stored in the `.losses` attribute of the model\")\n",
    "print(model3.losses)\n",
    "print(\"which is created by collecting all the `.losses` of the model's layers, e.g., a `DenseVariational` layer\") \n",
    "print(\"which is realized each time a forward pass during `.fit` actualizes the `DenseVariational` layer\")\n",
    "print(\"which is then used to calculate the KL penalty for the posterior against the specified prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(x_train, y_train[:, np.newaxis], epochs=100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'b.', label=\"train\")\n",
    "for i in range(50):\n",
    "  pred = model3(x_test)\n",
    "  plt.plot(x_test, pred.mean(), 'r', linewidth=0.5)\n",
    "plt.title(\"Regression with consideration for epistemic uncertainty\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh3dhmdkHNRp"
   },
   "source": [
    "## Model 4: Both aleatoric and epistemic uncertainty\n",
    "\n",
    "Combine the approaches in model 2 and model 3: \n",
    "\n",
    "* Estimate both the mean $\\theta$ and standard error $\\sigma$ using a 2-dimensonal dense layer\n",
    "\n",
    "* Use the DenseVariational layer instead of the fixed Dense layer to capture the variablity of the estimated parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdD8aww_JSDq",
    "outputId": "35306545-b90b-4f8a-a2c9-4b8a2a8bc5e7"
   },
   "outputs": [],
   "source": [
    "model4 = tf.keras.Sequential([\n",
    "                             tfp.layers.DenseVariational(2, posterior_mean_field, prior_trainable, kl_weight=1/x_train.shape[0]), \n",
    "                             tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,:1], \n",
    "                                                                                scale=t[:,1:]))\n",
    "])\n",
    "model4.compile(optimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik)\n",
    "model4.fit(x_train,y_train, epochs=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "eh4RjFElJgmH",
    "outputId": "fda7abf2-44f4-472a-cb48-3221de66d471"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'b.', label=\"train\")\n",
    "for i in range(30):\n",
    "  pred = model3(x_test)\n",
    "  pred_m = pred.mean()\n",
    "  pred_std = pred.stddev()\n",
    "  plt.plot(x_test, pred_m, \"r\", label = \"test\", linewidth=0.3)\n",
    "  plt.plot(x_test, pred_m + 2*pred_std, \"g\", label = \"+2 std\", linewidth=0.3)\n",
    "  plt.plot(x_test, pred_m - 2*pred_std, \"g\", label = \"-2 std\", linewidth=0.3)\n",
    "plt.title(\"Consider both the aleatoric and epistemic uncertainty\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hzu3bKtLSG_"
   },
   "source": [
    "Can get an ensemble mean and an ensemble variance of y for each value of x"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MADE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
